{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "url = \"https://pic.3gbizhi.com/\"\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url=url,headers=headers)\n",
    "response.encoding = response.apparent_encoding\n",
    "\n",
    "\"\"\"\n",
    ". 表示除空格外任意字符（除\\n外）\n",
    "* 表示匹配字符零次或多次\n",
    "? 表示匹配字符零次或一次\n",
    ".*? 非贪婪匹配\n",
    "\"\"\"\n",
    "parr = re.compile('src=\"(/u.*?)\".alt=\"(.*?)\"') # 匹配图片链接和图片名字\n",
    "image = re.findall(parr,response.text)\n",
    "\n",
    "path = \"3G壁纸\"\n",
    "if not os.path.isdir(path): # 判断是否存在该文件夹，若不存在则创建\n",
    "    os.mkdir(path) # 创建\n",
    "    \n",
    "# 对列表进行遍历\n",
    "for i in image:\n",
    "    link = i[0] # 获取链接\n",
    "    name = i[1] # 获取名字\n",
    "    \"\"\"\n",
    "    在文件夹下创建一个空jpg文件，打开方式以 'wb' 二进制读写方式\n",
    "    @param res：图片请求的结果\n",
    "    \"\"\"\n",
    "    with open(path+\"/{}.jpg\".format(name),\"wb\") as img:\n",
    "        res = requests.get(\"https://pic.3gbizhi.com\"+link)\n",
    "        img.write(res.content) # 将图片请求的结果内容写到jpg文件中\n",
    "        img.close() # 关闭操作\n",
    "    print(name+\".jpg 获取成功······\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import pymysql\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    " \n",
    "def get_conn():\n",
    "    # 创建连接\n",
    "    conn = pymysql.connect(host=\"127.0.0.1\",\n",
    "                           user=\"root\",\n",
    "                           password=\"root\",\n",
    "                           db=\"novels\",\n",
    "                           charset=\"utf8\")\n",
    "    # 创建游标\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor\n",
    " \n",
    "def close_conn(conn, cursor):\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    " \n",
    "def get_xpath_resp(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    tree = etree.HTML(resp.text)  # 用etree解析html\n",
    "    return tree,resp\n",
    " \n",
    "def get_chapters(url):\n",
    "    tree,_ = get_xpath_resp(url)\n",
    "    # 获取小说名字\n",
    "    novel_name = tree.xpath('//*[@id=\"info\"]/h1/text()')[0]\n",
    "    # 获取小说数据节点\n",
    "    dds = tree.xpath('/html/body/div[4]/dl/dd')\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "    for d in dds[:15]:\n",
    "        title = d.xpath('./a/text()')[0]  # 章节标题\n",
    "        title_list.append(title)\n",
    "        link = d.xpath('./a/@href')[0]   # 章节链接\n",
    "        chapter_url = url +link  # 构造完整链接\n",
    "        link_list.append(chapter_url)\n",
    "    return title_list,link_list,novel_name\n",
    " \n",
    "def get_content(novel_name,title,url):\n",
    "    try:\n",
    "        cursor = None\n",
    "        conn = None\n",
    "        conn, cursor = get_conn()\n",
    "        # 插入数据的sql\n",
    "        sql = 'INSERT INTO novel(novel_name,chapter_name,content) VALUES(%s,%s,%s)'\n",
    "        tree,resp = get_xpath_resp(url)\n",
    "        # 获取内容\n",
    "        content = re.findall('<div id=\"content\">(.*?)</div>',resp.text)[0]\n",
    "        # 对内容进行清洗\n",
    "        content = content.replace('<br />','\\n').replace('&nbsp;',' ').replace('全本小说网 www.qb5.tw，最快更新<a href=\"https://www.qb5.tw/book_116659/\">宇宙职业选手</a>最新章节！<br><br>','')\n",
    "        print(title,content)\n",
    "        cursor.execute(sql,[novel_name,title,content])  # 插入数据\n",
    "        conn.commit()  # 提交事务保存数据\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        sleep(2)\n",
    "        close_conn(conn, cursor)  # 关闭数据库\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # 获取小说名字，标题链接，章节名称\n",
    "    title_list, link_list, novel_name = get_chapters('https://www.qb5.tw/book_116659/')\n",
    "    with ThreadPoolExecutor(5) as t:  # 创建5个线程\n",
    "        for title,link in zip(title_list,link_list):\n",
    "            t.submit(get_content, novel_name,title,link)  # 启动线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "[Errno 11001] getaddrinfo failed\n",
      "[]\n",
      "第1条\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 90\u001b[0m\n\u001b[0;32m     85\u001b[0m     workbook\u001b[38;5;241m.\u001b[39msave(savepath)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     main()\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m爬取完毕\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m datelist \u001b[38;5;241m=\u001b[39m getDate(baseurl)\n\u001b[0;32m     13\u001b[0m savepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mjshk.xls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m saveDate(datelist,savepath)\n",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m, in \u001b[0;36msaveDate\u001b[1;34m(datalist, savepath)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m250\u001b[39m):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m第\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m条\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 81\u001b[0m     data\u001b[38;5;241m=\u001b[39mdatalist[i]\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     83\u001b[0m         worksheet\u001b[38;5;241m.\u001b[39mwrite(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,j,data[j])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "import xlwt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    baseurl =\"http://jshk.com.cn\"\n",
    "\n",
    "    datelist = getDate(baseurl)\n",
    "    savepath=\".\\\\jshk.xls\"\n",
    "    saveDate(datelist,savepath)\n",
    "\n",
    "    # askURL(\"http://jshk.com.cn/\")\n",
    "\n",
    "findlink = re.compile(r'<a href=\"(.*?)\">')\n",
    "findimg = re.compile(r'<img.*src=\"(.*?)\"',re.S)\n",
    "findtitle = re.compile(r'<span class=\"title\">(.*)</span')\n",
    "findrating = re.compile(r'<span class=\"rating_num\" property=\"v:average\">(.*)</span')\n",
    "findjudge = re.compile(r'<span>(\\d*)人评价</span>')\n",
    "findinq= re.compile(r'<span class=\"inq\">(.*)</span>')\n",
    "\n",
    "def getDate(baseurl):\n",
    "    datalist =[]\n",
    "    for i in range(0,10):\n",
    "        url=baseurl+str(i*25)\n",
    "        html=askURL(url)\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        for item in soup.find_all('div',class_=\"item\"):\n",
    "            data = []\n",
    "            item = str(item)\n",
    "            link = re.findall(findlink,item)[0]\n",
    "            data.append(link)\n",
    "            img=re.findall(findimg,item)[0]\n",
    "            data.append(img)\n",
    "            title=re.findall(findtitle,item)[0]\n",
    "\n",
    "            rating=re.findall(findrating,item)[0]\n",
    "            data.append(rating)\n",
    "            judge=re.findall(findjudge,item)[0]\n",
    "            data.append(judge)\n",
    "            inq=re.findall(findinq,item)\n",
    "\n",
    "            if len(inq)!=0:\n",
    "                inq=inq[0].replace(\"。\",\"\")\n",
    "                data.append(inq)\n",
    "            else:\n",
    "                data.append(\" \")\n",
    "            print(data)\n",
    "            datalist.append(data)\n",
    "        print(datalist)\n",
    "    return datalist\n",
    "\n",
    "def askURL(url):\n",
    "    head = { \n",
    "   \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"}\n",
    "    request=urllib.request.Request(url,headers=head)\n",
    "    html=\"\"\n",
    "    try:\n",
    "        response=urllib.request.urlopen(request)\n",
    "        html=response.read().decode(\"utf-8\")\n",
    "        # print(html)\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "\n",
    "    return html\n",
    "\n",
    "def saveDate(datalist,savepath):\n",
    "    workbook = xlwt.Workbook(encoding='utf-8')\n",
    "    worksheet = workbook.add_sheet('电影',cell_overwrite_ok=True)\n",
    "    col =(\"电影详情\",\"图片\",\"影片\",\"评分\",\"评价数\",\"概况\")\n",
    "    for i in range(0,5):\n",
    "        worksheet.write(0,i,col[i])\n",
    "    for i in range(0,250):\n",
    "        print(\"第%d条\" %(i+1))\n",
    "        data=datalist[i]\n",
    "        for j in range(0,5):\n",
    "            worksheet.write(i+1,j,data[j])\n",
    "\n",
    "    workbook.save(savepath)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"爬取完毕\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
